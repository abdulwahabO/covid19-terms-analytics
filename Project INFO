

	___________________________________________________________ Overview _____________________________________________________________
	
	 - Build a system that uses Kafka Streams to process a stream of Tweets and displays near real-time analytics on a site.
	 - Keep the business logic simple, it doesn't have to make sense in a real-world setting. 
	 - Main feature on frontend should be a grouped bar chart.
	 	 
	 - Analyze terminology used to describe the 2019 novel Coronavirus disease
	 
	__________________________________________________________ Tech __________________________________________________________________
	
			- Kafka
			- Spring (Boot, Data Mongo)
			- Maven (Use Kafka Streams archetype as shown in the docs tutorial to generate stream processor)
			- Docker
			- Github Actions
			- JUnit 5
			- Thymeleaf
			- ChartJS
			- MongoDB
	
	________________________________________________________ Actions _________________________________________________________________

	FINISH ENTIRE PROJECT USING EXACTLY 9 PULL REQUESTS:

	        DEADLINE: MAY 28th

	1. Setup the build of the entire project. Maven POMs and Github workflows........................DONE
	2. Identify and scaffold(with notes) all the classes/files/packages needed in each module........DONE
	3. Implement and test the common module..........................................................DONE
    4. Implement and test the tweet-extractor module.................................................STARTED
    5. Implement and test the tweet-processor module...........
    6. Implement and test the data-visualizer module..............
    7. Fix all issues from testing and resolve outstanding TO DOs ........
    8. Beautify the UI; Make screencast; Write a README....................
    9. Rename project and repository to covid19-analytis, package to c19a..................



	fourth PR______________________________________________________(4/9)________________________________________


    > implement Twitter API clients
    > implement Kafka Producer

	__________________________________________________________________________________________________________________

		
		> Figure out(and draw) the entire architecture of the project and begin to scaffold it.



		> Write all needed Dockerfiles..........
		> Write tests for the stream processing app using test utils module provide by Kafka.

		> See other data eng projects for inspiration on writing a good README.
		> Draw architecture, described Topology for processing.

		> Configure Kafka to ensure exactly-once delivery semantics.

		> Write a Test program to confirm that Okhttp supports the kind of streaming response twitter send.
		
		> Write a small program to test use of a properties file in /resources and loading into a Properties object.
			--> Test too that the file will resolve ${} using maven environment config.
			--> if it works: More options(e.g object values) can then be added to the Properties object as needed 
			
		> At end of project implement Kafka security for streams processor, producer and consumer.
		
		Notes on frontend:
			-- Use SSEmitter class with controller.. NO WEBSOCKETS, since messages are uni-directional.
			-- Don't use any framework at all. USe EventSource API to consume SSE
			-- see ChartJS documentation(other people's demos) for how to build grouped bar charts and update live data.

	
	_______________________________________________ Architecture _____________________________________________________________________
	
	Three applications (1 web app, 2 executable JARs with main method) and one library:
		
		1. Extractor (stream producer).. Extracts tweets from Twitter's API and write them to the Input Topic.
		2. The stream processing app... Reads tweets from input topic, applies transformations and writes to Output Topic.
		3. Web app: reads from output topic into buffer records, updates frontend via SSE and writes to Database.
		4. [Library]: a common module that would contain:
						-- Maven dependency for "kafka-clients" 
						-- definition for DTOs and the custom JSON Serdes used in Kafka code. 
		
	______________________________________________________ Business Logic ____________________________________________________________
	
	Use Twitter's Fitered Streaming API to
	
		--> Use replies and original tweets.. NO retweets.
	
		--> Track use of 5 terms for COVID19 and group by number of followers the users have.

		    x-axis : Terminologies, e.g Sars-CoV-2, COVID19, Chinese virus, Wuhan Virus, Corona Virus.
		    y-axis : Percent, 0 - 100

		OBJECTIVE: Show the percentage of tweets that use each term among verified and unverified users.
		----> E.g for 'Corona Virus' we could have 39% of unverified users and 27% of verified users.
		
		Use Aggregate operations provided by Streams DSL or Implement new ones using Processor API:
			-- create a KGroupedTable, using virus name as keys, country codes as values and number of tweets as row data.
			i.e K,V store where K = term for pandemic,
				 V = an object which has the term has field and a Map<CountryCode, number> as another field
					This way country codes are not hardcoded, but can still be sent over the wire as JSON.
					
					Transformation:
					KStream has {term: "Chinese Virus", country: "US"} --> sent by producer in real time.
						--> First groupByKey
						-> then aggregate() into KTable by using _term_ as key, since the aggregate value is a map type update it.

				INITIALIZER for the KGroupedTable should layout the entire schema with zero values.
						 
						 			
		
					The web app that consumes the output topic can then calculate the percentages to be displayed.
		
_____________________________________________ AWS Deployment ________________________________________________________

 > After testing local and writing README. Deploy to cloud using Github Actions?

 > Describe the AWS architecture in README.

 > Allow it run for 3/4 days and make screencasts.

 > Tear down properly to AWS avoid costs.
