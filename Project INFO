

	___________________________________________________________ Overview _____________________________________________________________

	 
	__________________________________________________________ Tech __________________________________________________________________
	
			- Kafka
			- Spring (Boot, Data Mongo)
			- Maven (Uselogger for Java Kafka Streams archetype as shown in the docs tutorial to generate stream processor)
			- Docker
			- Github Actions
			- JUnit 5
			- Thymeleaf
			- ChartJS
	
	________________________________________________________ Actions _________________________________________________________________

    0. Obtain twitter app credentials.
    1. Test the whole System manually.
    2. Beautify the UI; Make screencast; Write a README.
    3. Rename project and repository to covid19-tweets-analytis, package to c19a
	__________________________________________________________________________________________________________________

        > Also use Generics where possible.

		> Write tests for the stream processing app using test utils module provide by Kafka.

		> See other data eng projects for inspiration on writing a good README.
		> Draw architecture, describe Topology for processing.

		> Configure Kafka to ensure exactly-once delivery semantics.
		> Build extractor and stream processor with `mvn -B package`

		> Delete this file and Manual Test Plan.

	_______________________________________________ Architecture _____________________________________________________________________
	
	Three applications (1 web app, 2 executable JARs with main method) and one library:
		
		1. Extractor (stream producer).. Extracts tweets from Twitter's API and write them to the Input Topic.
		2. The stream processing app... Reads tweets from input topic, applies transformations and writes to Output Topic.
		3. Web app: reads from output topic into buffer records, updates frontend via SSE and writes to Database.
		4. [Library]: a common module that would contain:
						-- Maven dependency for "kafka-clients" 
						-- definition for DTOs and the custom JSON Serdes used in Kafka code. 
		
	______________________________________________________ Business Logic ____________________________________________________________
	
	Use Twitter's Filtered Streaming API to
	
		--> Use replies and original tweets.. NO retweets.
	
		--> Track use of 5 terms for COVID19 and group by number of followers the users have.

		    x-axis : Terminologies, e.g Sars-CoV-2, COVID19, Chinese virus, Wuhan Virus, Corona Virus.
		    y-axis : Percent, 0 - 100

		OBJECTIVE: Show the percentage of tweets that use each term among verified and unverified users.
		----> E.g for 'Corona Virus' we could have 39% of unverified users and 27% of verified users.
		
		Use Aggregate operations provided by Streams DSL or Implement new ones using Processor API:
			-- create a KGroupedTable, using virus name as keys, country codes as values and number of tweets as row data.
			i.e K,V store where K = term for pandemic,
				 V = an object which has the term has field and a Map<CountryCode, number> as another field
					This way country codes are not hardcoded, but can still be sent over the wire as JSON.
					
					Transformation:
					KStream has {term: "Chinese Virus", is_verified: true} --> sent by producer in real time.
						--> First groupByKey
						-> then aggregate() into KTable by using _term_ as key, since the aggregate value is a map type update it.

				INITIALIZER for the KGroupedTable should layout the entire schema with zero values.

					The web app that consumes the output topic can then calculate the percentages to be displayed.

